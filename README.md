<p align="center">
  <h1 align="center">ğŸƒ RL_phase2 â€” AI Poker Agent Framework</h1>
  <p align="center">
    <strong>Deep Reinforcement Learning & Supervised Learning pour le No-Limit Texas Hold'em 6-max</strong>
  </p>
  <p align="center">
    <img src="https://img.shields.io/badge/Python-3.10%2B-blue?logo=python&logoColor=white" alt="Python 3.10+">
    <img src="https://img.shields.io/badge/PyTorch-2.0%2B-ee4c2c?logo=pytorch&logoColor=white" alt="PyTorch">
    <img src="https://img.shields.io/badge/RLCard-1.0.5-green" alt="RLCard">
    <img src="https://img.shields.io/badge/XGBoost-1.7.6-blue?logo=xgboost" alt="XGBoost">
    <img src="https://img.shields.io/badge/SB3--Contrib-MaskablePPO-orange" alt="SB3-Contrib">
    <a href="LICENSE"><img src="https://img.shields.io/badge/Licence-CC%20BY--NC%204.0-red" alt="Licence CC BY-NC 4.0"></a>
  </p>
</p>

---

## âš ï¸ **Avertissement Ã‰thique & Usage Responsable**

> [!CAUTION]
> **Ce projet est STRICTEMENT rÃ©servÃ© Ã  la recherche acadÃ©mique en intelligence artificielle et thÃ©orie des jeux.**
> Toute utilisation commerciale ou Ã  des fins de triche est **formellement interdite**.

Ce projet est destinÃ© **exclusivement** Ã  la recherche en intelligence artificielle, thÃ©orie des jeux et mathÃ©matiques appliquÃ©es.

### ğŸš« Usages interdits

| Usage | Statut |
|---|---|
| Triche ou aide en temps rÃ©el sur des sites de poker en ligne | **âŒ INTERDIT** |
| Exploitation commerciale (vente, SaaS, API payante) | **âŒ INTERDIT** |
| Contournement des CGU des plateformes de poker | **âŒ INTERDIT** |
| DÃ©veloppement de bots jouant avec de l'argent rÃ©el | **âŒ INTERDIT** |
| Recherche acadÃ©mique, Ã©ducation, expÃ©rimentation personnelle | âœ… AutorisÃ© |
| Publication scientifique (avec citation) | âœ… AutorisÃ© |

### âš–ï¸ ConformitÃ© lÃ©gale

- Les plateformes de poker en ligne (PokerStars, GGPoker, Winamax, etc.) **interdisent explicitement** l'utilisation d'outils d'aide Ã  la dÃ©cision en temps rÃ©el. Tout contrevenant s'expose Ã  la **fermeture de compte** et Ã  des **poursuites lÃ©gales**.
- Les modÃ¨les entraÃ®nÃ©s sur des donnÃ©es Pluribus sont soumis aux licences des auteurs originaux (Meta AI Research).
- L'utilisation de ce logiciel doit respecter les **lois locales** applicables aux jeux d'argent.
- Ce projet est distribuÃ© sous licence **[CC BY-NC 4.0](LICENSE)** â€” voir la section [Licence](#-licence).

---

## ğŸ™ Remerciements

Ce travail repose sur les contributions de la communautÃ© scientifique et open-source :

### DonnÃ©es d'entraÃ®nement
- **Pluribus** (Meta/Facebook AI Research) :
  DonnÃ©es de parties publiÃ©es par [Noam Brown et Tuomas Sandholm](https://science.sciencemag.org/content/365/6456/885) (Science, 2019).
  AccÃ¨s via le projet **[Poker-Hand-History](https://github.com/uoftcprg/poker-hand-history)** (University of Toronto CPRG).

### Outils open-source
- [RLCard](https://github.com/datamllab/rlcard) (environnement poker pour RL)
- [Treys](https://github.com/ihendley/treys) (Ã©valuation de mains)
- [Stable-Baselines3](https://github.com/DLR-RM/stable-baselines3) (implÃ©mentation PPO/DQN)

---

## ğŸ“‹ Table des matiÃ¨res

- [Vue d'ensemble](#-vue-densemble)
- [Architecture](#-architecture)
- [FonctionnalitÃ©s](#-fonctionnalitÃ©s)
- [Installation](#-installation)
- [DÃ©marrage rapide](#-dÃ©marrage-rapide)
- [Modules en dÃ©tail](#-modules-en-dÃ©tail)
  - [Core â€” GameState](#core--gamestate)
  - [Agents](#agents)
  - [Features â€” Feature Engineering](#features--feature-engineering)
  - [Adapters â€” Multi-source](#adapters--multi-source)
  - [Training â€” EntraÃ®nement RL](#training--entraÃ®nement-rl)
  - [Parsers â€” DonnÃ©es Pluribus](#parsers--donnÃ©es-pluribus)
- [Pipeline d'entraÃ®nement](#-pipeline-dentraÃ®nement)
- [Live Assistant â€” Aide en temps rÃ©el](#-live-assistant--aide-en-temps-rÃ©el)
- [Ã‰valuation & Tournois](#-Ã©valuation--tournois)
- [Configuration](#%EF%B8%8F-configuration)
- [Roadmap](#-roadmap)
- [Stack technologique](#-stack-technologique)
- [Licence](#-licence)
- [Contribuer](#-contribuer)

---

## ğŸ¯ Vue d'ensemble

**RL_phase2** est un framework de recherche combinant :
- **Apprentissage par Renforcement** (DQN, MaskablePPO)
- **Apprentissage SupervisÃ©** (XGBoost entraÃ®nÃ© sur les donnÃ©es de [Pluribus](https://science.fb.com/publications/superhuman-ai-for-multiplayer-poker/))

pour crÃ©er des agents compÃ©titifs au **No-Limit Texas Hold'em 6-max**.

### Philosophie du projet

```text
DonnÃ©es Pluribus (.phh)  â”€â”€â–º  XGBoost (Imitation Learning)  â”€â”€â–º  Adversaire expert
                                                                        â”‚
Environnement RLCard     â”€â”€â–º  DQN / MaskablePPO (RL)        â”€â”€â–º  Agent  â”‚ vs
                                                                        â”‚
                              Feature Extractor (87 dims)    â—„â”€â”€  GameState standardisÃ©
```

## ğŸ— Architecture

```
RL_phase2/
â”œâ”€â”€ core/                    # ğŸ”´ Noyau â€” Structures de donnÃ©es
â”‚   â””â”€â”€ game_state.py        #     GameState : Ã©tat standardisÃ© du jeu
â”‚
â”œâ”€â”€ agents/                  # ğŸŸ¢ Agents IA
â”‚   â”œâ”€â”€ dqn.py               #     SmartDQNAgent (RLCard + Features)
â”‚   â”œâ”€â”€ ppo.py               #     PPOAgent (Gym wrapper pour SB3)
â”‚   â”œâ”€â”€ xgboost_agent.py     #     XGBoostRLCardAgent (Pluribus-trained)
â”‚   â”œâ”€â”€ human_console.py     #     Agent humain (interface console)
â”‚   â””â”€â”€ dqn_training.py      #     Script de lancement DQN
â”‚
â”œâ”€â”€ features/                # ğŸŸ¡ Feature Engineering
â”‚   â”œâ”€â”€ feature_builder.py   #     FeatureExtractor (87 features)
â”‚   â””â”€â”€ feature_sota_drl.py  #     Features SOTA pour DRL
â”‚
â”œâ”€â”€ adapters/                # ğŸ”µ Adaptateurs multi-sources
â”‚   â”œâ”€â”€ rlcard_adapter.py    #     RLCard â†’ GameState
â”‚   â””â”€â”€ pluribus_adapter.py  #     Pluribus (.phh) â†’ GameState
â”‚
â”œâ”€â”€ training/                # ğŸŸ  Infrastructure d'entraÃ®nement
â”‚   â”œâ”€â”€ config.py            #     Configurations type-safe (dataclasses)
â”‚   â”œâ”€â”€ trainer.py           #     PokerRLTrainer (agnostique)
â”‚   â”œâ”€â”€ trainer_sb3.py       #     EntraÃ®neur MaskablePPO (SB3)
â”‚   â”œâ”€â”€ sb3wrapper.py        #     PokerSB3Wrapper (Gymnasium env)
â”‚   â””â”€â”€ callbacks.py         #     Callbacks (Progress, Metrics, EarlyStopping, TensorBoard)
â”‚
â”œâ”€â”€ parsers/                 # ğŸŸ£ Parsing de donnÃ©es
â”‚   â”œâ”€â”€ phh_parsers.py       #     Parser Pluribus Hand History (.phh)
â”‚   â””â”€â”€ parse_data.py        #     Utilitaires de conversion
â”‚
â”œâ”€â”€ data/                    # ğŸ“¦ DonnÃ©es
â”‚   â”œâ”€â”€ script_pluribus.ipynb#     Notebook d'exploration Pluribus
â”‚   â””â”€â”€ xgb_pluribus_v1.pkl  #     ModÃ¨le XGBoost prÃ©-entraÃ®nÃ©
â”‚
â”œâ”€â”€ tests/                   # ğŸ§ª Tests & Benchmarks
â”‚   â”œâ”€â”€ test_agent.py        #     Tests des agents
â”‚   â”œâ”€â”€ test_feature_builder.py#   Tests du feature extractor
â”‚   â”œâ”€â”€ test_sb3_env.py      #     Tests de l'env Gymnasium
â”‚   â”œâ”€â”€ test_params.py       #     Tests des hyperparamÃ¨tres
â”‚   â”œâ”€â”€ tournament.py        #     Tournoi XGBoost vs DQN
â”‚   â””â”€â”€ explore_rlcard.py    #     Exploration de l'API RLCard
â”‚
â”œâ”€â”€ live_poker_pro.py        # ğŸ° Assistant live (Rich UI)
â”œâ”€â”€ live_assistant.py        # ğŸ° Assistant live (CLI)
â”œâ”€â”€ play_human.py            # ğŸ•¹ï¸  Mode Humain vs IA
â”œâ”€â”€ plot_results.py          # ğŸ“Š Visualisation TensorBoard
â”œâ”€â”€ convert_to_json.py       # ğŸ”„ Conversion modÃ¨le pkl â†’ json
â””â”€â”€ requirements.txt         # ğŸ“‹ DÃ©pendances Python
```

---

## âœ¨ FonctionnalitÃ©s

| FonctionnalitÃ© | Description | Status |
|---|---|---|
| ğŸ§  **SmartDQN** | Agent DQN avec features intelligentes (override de RLCard) | âœ… |
| ğŸš€ **MaskablePPO** | PPO avec action masking via SB3-Contrib | âœ… |
| ğŸŒ² **XGBoost Agent** | Imitation learning Ã  partir de Pluribus | âœ… |
| ğŸ¯ **87 Features** | Feature engineering avancÃ© (cartes, position, GTO) | âœ… |
| ğŸ”„ **Adapters** | Support multi-source (RLCard + Pluribus .phh) | âœ… |
| âš™ï¸ **Config system** | Configurations type-safe avec presets (quick, standard, Kaggle) | âœ… |
| ğŸ“Š **TensorBoard** | Monitoring temps rÃ©el des mÃ©triques | âœ… |
| ğŸ° **Live Assistant** | Assistant poker en temps rÃ©el (Rich UI) | âœ… |
| ğŸ•¹ï¸ **Human vs AI** | Mode de jeu humain contre les agents | âœ… |
| ğŸ† **Tournois** | Benchmark automatisÃ© entre agents | âœ… |
| ğŸ’¾ **Checkpointing** | Sauvegarde/reprise device-agnostic (CPU â†” GPU) | âœ… |
| ğŸ” **Callbacks** | EarlyStopping, MetricsCallback, SmartCheckpoint | âœ… |
| ğŸ“ **Multi-CPU** | EntraÃ®nement parallÃ¨le via `SubprocVecEnv` | âœ… |
| ğŸ¤– **Self-play** | Configuration prÃªte pour le self-play | ğŸ”œ |

---

## ğŸš€ Installation

### PrÃ©requis

- Python **3.10+**
- pip ou conda
- (Optionnel) GPU CUDA pour l'entraÃ®nement accÃ©lÃ©rÃ©

### Setup

```bash
# 1. Cloner le repository
git clone https://github.com/<votre-user>/RL_phase2.git
cd RL_phase2

# 2. CrÃ©er un environnement virtuel
python -m venv venv
source venv/bin/activate  # macOS/Linux
# venv\Scripts\activate   # Windows

# 3. Installer les dÃ©pendances
pip install -r requirements.txt
```

### DÃ©pendances principales

| Package | Version | RÃ´le |
|---|---|---|
| `torch` | â‰¥ 2.0.0 | RÃ©seaux de neurones (DQN) |
| `rlcard` | â‰¥ 1.0.5 | Environnement de poker |
| `xgboost` | 1.7.6 | Agent supervisÃ© |
| `sb3-contrib` | latest | MaskablePPO |
| `treys` | 0.1.8 | Ã‰valuation de mains |
| `rich` | 14.3.1 | Interface terminal |
| `matplotlib` | â‰¥ 3.7.0 | Visualisation |
| `scikit-learn` | â‰¥ 1.3.0 | Utilitaires ML |
| `imbalanced-learn` | â‰¥ 0.11.0 | RÃ©Ã©quilibrage classes |
| `pandas` | â‰¥ 2.0.0 | Manipulation de donnÃ©es |

---

## âš¡ DÃ©marrage rapide

### 1. Jouer contre l'IA (Humain vs XGBoost)

```bash
python play_human.py
```

Vous incarnez le Joueur 5 (position BTN) face Ã  un agent XGBoost entraÃ®nÃ© sur les donnÃ©es Pluribus, accompagnÃ© de 4 joueurs alÃ©atoires.

### 2. Lancer l'assistant poker live

```bash
# Version Rich (interface premium)
python live_poker_pro.py

# Version CLI (terminal simple)
python live_assistant.py
```

L'assistant analyse votre main en temps rÃ©el et vous conseille l'action optimale avec niveaux de confiance.

### 3. EntraÃ®ner un agent DQN

```bash
python agents/dqn_training.py
```

### 4. EntraÃ®ner un agent PPO (MaskablePPO)

```bash
python training/trainer_sb3.py
```

Lance l'entraÃ®nement parallÃ¨le sur tous les cÅ“urs CPU avec sauvegarde automatique et TensorBoard.

### 5. Lancer un tournoi d'Ã©valuation

```bash
python tests/tournament.py
```

Compare les agents XGBoost vs DQN sur 10 000 mains.

### 6. Visualiser les rÃ©sultats

```bash
# Via le script intÃ©grÃ©
python plot_results.py

# Ou via TensorBoard
tensorboard --logdir=logs
```

---

## ğŸ“¦ Modules en dÃ©tail

### Core â€” GameState

Le cÅ“ur du systÃ¨me est le `GameState` â€” une dataclass Python qui standardise l'Ã©tat du jeu, **indÃ©pendamment de la source** (RLCard ou Pluribus).

```python
@dataclass
class GameState:
    hole_cards: List[str]        # ['As', 'Kd']
    board: List[str]             # ['Jh', '9c', '4s']
    street: str                  # 'preflop' | 'flop' | 'turn' | 'river'
    position: str                # 'BTN' | 'SB' | 'BB' | 'UTG' | 'MP' | 'CO'
    num_active_players: int
    pot_size: int
    stack: int
    big_blind: int
    amount_to_call: int
    legal_actions: List[str]
    actions_this_street: List[str]
```

**PropriÃ©tÃ©s calculÃ©es dynamiquement** : `effective_stack_bb`, `pot_odds`, `spr` (Stack-to-Pot Ratio), `is_all_in_situation`, `is_heads_up`.

---

### Agents

#### SmartDQNAgent (`agents/dqn.py`)

Ã‰tend le `DQNAgent` de RLCard en injectant le `FeatureExtractor` custom Ã  la place des observations brutes. Supporte checkpoint device-agnostic (CPU â†” GPU).

```python
agent = SmartDQNAgent(
    env=env,
    mlp_layers=[256, 128],
    replay_memory_size=50000,
    learning_rate=0.00005,
    epsilon_decay_steps=100000
)
```

#### XGBoostRLCardAgent (`agents/xgboost_agent.py`)

Agent supervisÃ© entraÃ®nÃ© par imitation learning sur les dÃ©cisions de Pluribus. Supporte les formats `.pkl`, `.json` et `.ubj`.

```python
agent = XGBoostRLCardAgent(
    model_path='models/xgb/xgb_pluribus_V1.json',
    env=env,
    use_safe_mode=True  # Fallback sÃ©curisÃ© en cas d'erreur
)
```

**StratÃ©gie de mapping intelligente** :
- `RAISE` non disponible + conviction > 70% â†’ `ALL-IN`
- `RAISE` non disponible + conviction â‰¤ 70% â†’ `CALL`

#### PPOAgent (`agents/ppo.py`)

Wrapper Gym pour utiliser PPO/MaskablePPO via Stable-Baselines3. Inclut un `LegalActionWrapper` qui masque les actions illÃ©gales pendant l'infÃ©rence.

---

### Features â€” Feature Engineering

Le `FeatureExtractor` (`features/feature_builder.py`) produit un vecteur de **87 features** Ã  partir de n'importe quel `GameState` :

| CatÃ©gorie | # Features | Exemples |
|---|---|---|
| ğŸƒ **Cartes** | 22 | Rank, suited, paire, force de main (Treys), texture du board |
| ğŸ“ **Position** | 6 | Position normalisÃ©e, distance au BTN, early/middle/late |
| ğŸ’° **Stack & Pot** | 12 | SPR, pot odds, stack en BB, catÃ©gorie short/deep |
| ğŸ¬ **Actions** | 15 | Historique d'aggression, facing bet, actions lÃ©gales |
| ğŸŒ **Contexte** | 12 | Street one-hot, nombre de joueurs, heads-up |
| ğŸ§® **Game Theory** | 20 | EV estimÃ©, fold equity, equity, implied odds, polarisation |

L'Ã©valuation de la force de main utilise la bibliothÃ¨que **Treys** pour un calcul rapide et prÃ©cis.

---

### Adapters â€” Multi-source

Le pattern **Adapter** permet d'alimenter le mÃªme `FeatureExtractor` depuis des sources diffÃ©rentes :

```
RLCard env.step()  â”€â”€â–º  RLCardAdapter.to_game_state()   â”€â”€â–º  GameState
Pluribus .phh      â”€â”€â–º  PluribusAdapter.to_game_state()  â”€â”€â–º  GameState
```

- **`RLCardAdapter`** : GÃ¨re les positions (SB/BB/UTG/MP/CO/BTN), convertit les cartes (uppercase â†’ capitalize), calcule `amount_to_call` depuis `all_chips`.
- **`PluribusAdapter`** : Parse les donnÃ©es `.phh`, infÃ¨re la position du joueur, reconstruit l'historique d'actions par street.

---

### Training â€” EntraÃ®nement RL

#### PokerRLTrainer (`training/trainer.py`)

Trainer **agnostique** compatible avec tout agent implÃ©mentant `feed()` (DQN) ou `update()` (PPO).

```python
from training.config import get_standard_training_config

config = get_standard_training_config()
trainer = PokerRLTrainer(agent=agent, config=config, callbacks=[...])
trainer.train()
```

#### MaskablePPO Trainer (`training/trainer_sb3.py`)

Pipeline **production-ready** intÃ©grant :
- EntraÃ®nement **multi-CPU** via `SubprocVecEnv`
- **Action masking** natif avec `MaskablePPO`
- Sauvegarde automatique avec mÃ©tadonnÃ©es JSON
- Visualisation TensorBoard en fin d'entraÃ®nement

#### Callbacks (`training/callbacks.py`)

| Callback | RÃ´le |
|---|---|
| `ProgressCallback` | Progression temps rÃ©el (eps/s, ETA) |
| `MetricsCallback` | Enregistrement et graphiques (reward, win_rate, loss) |
| `EarlyStoppingCallback` | ArrÃªt si plateau (patience configurable) |
| `CheckpointCallback` | Sauvegarde pÃ©riodique + best model |
| `TensorBoardCallback` | Logging vers TensorBoard |
| `SmartCheckpointCallback` | Pour SB3, sauvegarde avec versioning |

#### PokerSB3Wrapper (`training/sb3wrapper.py`)

Environnement Gymnasium complet avec :
- **Action masking** (`action_masks()`) pour MaskablePPO
- Gestion du tour de jeu multi-joueurs (`_play_until_my_turn`)
- Support adversaires XGBoost ou Random

---

### Parsers â€” DonnÃ©es Pluribus

Le `PHHParser` (`parsers/phh_parsers.py`) extrait les dÃ©cisions de jeu depuis les fichiers `.phh` (Pluribus Hand History) contenus dans une archive ZIP.

```python
parser = PHHParser("data/poker-hand-histories.zip")
hands = parser.parse_all(max_hands=10000)
# Chaque main contient les dÃ©cisions (Ã©tat + action) pour l'entraÃ®nement supervisÃ©
```

---

## ğŸ”„ Pipeline d'entraÃ®nement

```mermaid
graph LR
    A["ğŸ“‚ Pluribus .phh"] -->|PHHParser| B["ğŸ“Š Dataset"]
    B -->|XGBoost| C["ğŸŒ² Agent XGB"]
    
    D["ğŸ® RLCard Env"] -->|PokerRLTrainer| E["ğŸ§  Agent DQN"]
    D -->|SB3 Wrapper| F["ğŸš€ Agent PPO"]
    
    C -->|Adversaire| D
    
    E --> G["ğŸ† Tournament"]
    F --> G
    C --> G
    
    G -->|Ã‰valuation| H["ğŸ“ˆ MÃ©triques"]
    H -->|TensorBoard| I["ğŸ“Š Visualisation"]
```

### Workflow typique

1. **Phase 1 â€” Imitation Learning** : Parser les donnÃ©es Pluribus â†’ EntraÃ®ner XGBoost â†’ Agent baseline expert.
2. **Phase 2 â€” Reinforcement Learning** : EntraÃ®ner DQN/PPO contre l'agent XGBoost (curriculum learning).
3. **Phase 3 â€” Ã‰valuation** : Tournois entre agents sur des milliers de mains â†’ Analyse statistique.

---

## ğŸ° Live Assistant â€” Aide en temps rÃ©el

L'assistant live permet d'obtenir les **recommandations de l'IA en temps rÃ©el** pendant une partie de poker (en ligne ou live).

### Live Poker Pro (Rich UI)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        ğŸ° LIVE POKER ASSISTANT - XGBOOST     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ MA MAIN: Aâ™  Kâ™¦  | BOARD: Jâ™¥ 9â™£ 4â™            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ğŸ’° Pot: 500 | ğŸ’¸ A payer: 200 | ğŸ“ BTN      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ğŸ¤– CONSEIL: ğŸš€ RAISE                         â”‚
â”‚   RAISE       62.3%  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          â”‚
â”‚   CHECK/CALL  28.1%  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                  â”‚
â”‚   FOLD         9.6%  â–ˆâ–ˆ                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Commandes** : Entrez vos cartes (`AhKd`), le board (`JH 9C 4S`), ajustez le pot (`p 500`), position (`pos 5`), puis l'IA analyse.

---

## ğŸ† Ã‰valuation & Tournois

```bash
# Tournoi automatisÃ© (10 000 mains)
python tests/tournament.py
```

**MÃ©triques clÃ©s** :
- **BB/main** (Big Blinds par main) â€” mesure standard de profitabilitÃ©
- **Win rate** â€” pourcentage de mains gagnantes
- **Distribution d'actions** â€” FOLD/CALL/RAISE/ALL-IN
- **Comparaison** vs Random, vs XGBoost, vs DQN

**Seuils d'interprÃ©tation** :
| RÃ©sultat | InterprÃ©tation |
|---|---|
| BB/main > 0 | âœ… Agent profitable |
| BB/main > -0.25 | âš ï¸ Correct (mieux que fold systÃ©matique) |
| BB/main < -0.25 | âŒ Agent exploitable |

---

## âš™ï¸ Configuration

Le systÃ¨me de configuration utilise des **dataclasses Python** pour une configuration type-safe et flexible.

### Presets disponibles

```python
from training.config import (
    get_quick_test_config,       # 1 000 Ã©pisodes (~5 min)
    get_standard_training_config, # 50 000 Ã©pisodes
    get_kaggle_config,           # 100 000 Ã©pisodes (GPU, optimisÃ© 12h)
    get_self_play_config         # 100 000 Ã©pisodes en self-play
)
```

### Configuration custom

```python
from training.config import FullTrainingConfig, TrainingConfig, OpponentConfig

config = FullTrainingConfig(
    training=TrainingConfig(
        num_episodes=25000,
        eval_every=2000,
        device='cuda',
        experiment_name='mon_experience'
    ),
    opponent=OpponentConfig(
        type='xgboost',
        model_path='models/xgb/mon_modele.json',
        num_opponents=5
    )
)

# Sauvegarder / Charger
config.save('configs/ma_config.json')
config = FullTrainingConfig.load('configs/ma_config.json')
```

---

## ğŸ—º Roadmap

- [x] GameState standardisÃ© + Adapter pattern
- [x] Feature Extractor (87 features + Treys)
- [x] Agent XGBoost (Imitation Learning Pluribus)
- [x] Agent SmartDQN (RLCard)
- [x] Agent MaskablePPO (Stable-Baselines3)
- [x] SystÃ¨me de callbacks complet
- [x] Live assistant (Rich UI)
- [x] Tournois & benchmarks
- [ ] Self-play training loop complet
- [ ] Population-based training (PBT)
- [ ] Monte Carlo CFR (Counterfactual Regret Minimization)
- [ ] IntÃ©gration W&B (Weights & Biases)
- [ ] Agent transformer (attention-based)
- [ ] DÃ©ploiement API (FastAPI / gRPC)

---

## ğŸ›  Stack technologique

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    APPLICATION LAYER                â”‚
â”‚  live_poker_pro.py â”‚ play_human.py â”‚ tournament.py  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                     AGENT LAYER                     â”‚
â”‚    SmartDQNAgent  â”‚  MaskablePPO  â”‚  XGBoostAgent   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                    FEATURE LAYER                    â”‚
â”‚         FeatureExtractor (87 dims) + Treys          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                    ADAPTER LAYER                    â”‚
â”‚        RLCardAdapter  â”‚  PluribusAdapter            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                     CORE LAYER                      â”‚
â”‚              GameState (dataclass)                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                  ENVIRONMENT LAYER                  â”‚
â”‚           RLCard  â”‚  Gymnasium  â”‚  SB3              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                  INFRASTRUCTURE                     â”‚
â”‚    PyTorch â”‚ XGBoost â”‚ NumPy â”‚ TensorBoard â”‚ Rich   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“„ Licence

Ce projet est distribuÃ© sous la licence **Creative Commons Attribution - Pas d'Utilisation Commerciale 4.0 International** ([CC BY-NC 4.0](https://creativecommons.org/licenses/by-nc/4.0/deed.fr)).

### Ce que cela signifie :

| Vous pouvez | Vous ne pouvez PAS |
|---|---|
| âœ… Utiliser pour la recherche | âŒ Vendre ou commercialiser |
| âœ… Modifier et adapter | âŒ CrÃ©er un service payant |
| âœ… Partager (avec attribution) | âŒ Utiliser pour tricher au poker |
| âœ… Publier des travaux dÃ©rivÃ©s (non-commerciaux) | âŒ Retirer la mention d'attribution |

Voir le fichier [LICENSE](LICENSE) pour le texte complet.

---

## ğŸ¤ Contribuer

1. Fork le projet
2. CrÃ©er une branche feature (`git checkout -b feature/mon-agent`)
3. Commit les changements (`git commit -m 'feat: ajout agent MCTS'`)
4. Push (`git push origin feature/mon-agent`)
5. Ouvrir une Pull Request

---

<p align="center">
  <em>Recherche en mathÃ©matiques appliquÃ©es / IA â€” Killian GUILLAUME</em>
</p>

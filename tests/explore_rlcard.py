"""
D√©couvrir comment fonctionne RLCard.
Ce fichier est juste pour comprendre, pas pour produire.
"""

import rlcard
from rlcard.agents import RandomAgent
from rlcard.utils import set_seed, tournament
import numpy as np

print("=" * 60)
print("üÉè EXPLORATION DE RLCARD")
print("=" * 60)

# Initialiser l'environnement
set_seed(42)
env = rlcard.make('no-limit-holdem', config={'seed': 42})


print("--- D√âCODAGE DES ACTIONS ---")
for action_id in range(env.num_actions):
    try:
        decoded = env.decode_action(action_id)
        print(f"Action {action_id} correspond √† : {decoded}")
    except:
        pass

# Ajouter 2 agents al√©atoires
env.set_agents([
    RandomAgent(num_actions=env.num_actions),
    RandomAgent(num_actions=env.num_actions)
])

print(f"\nNombre d'actions possibles: {env.num_actions}")
print(f"Forme de l'√©tat: {env.state_shape}")
print(f"Actions: {env.actions}")

# Jouer une main et voir ce qui se passe
print("\n--- SIMULATION D'UNE MAIN ---")
trajectories, payoffs = env.run(is_training=False)

print(f"\n‚úÖ R√©sultat: Joueur 0 = {payoffs[0]}, Joueur 1 = {payoffs[1]}")

# Explorer la structure des trajectoires
print("\n--- STRUCTURE DES TRAJECTOIRES ---")
print(f"Type de trajectories: {type(trajectories)}")
print(f"Nombre de joueurs: {len(trajectories)}")
print(f"Type de trajectories[0]: {type(trajectories[0])}")
print(f"Longueur de trajectories[0]: {len(trajectories[0])}")

# Premi√®re d√©cision du joueur 0
print("\n--- PREMI√àRE D√âCISION DU JOUEUR 0 ---")
first_decision = trajectories[0][0]

print(f"Type de first_decision: {type(first_decision)}")

# V√©rifier si c'est un dictionnaire
if isinstance(first_decision, dict):
    print(f"Cl√©s disponibles: {list(first_decision.keys())}")
    print(f"\n√âtat re√ßu (shape): {np.array(first_decision['obs']).shape}")
    print(f"√âtat re√ßu (5 premi√®res valeurs): {np.array(first_decision['obs'])[:5]}")
    print(f"Actions l√©gales: {first_decision['legal_actions']}")
else:
    print(f"‚ö†Ô∏è Ce n'est pas un dictionnaire, c'est: {first_decision}")

# Analyser TOUS les √©l√©ments de la trajectoire
print("\n--- ANALYSE COMPL√àTE DE LA TRAJECTOIRE DU JOUEUR 0 ---")
for i, element in enumerate(trajectories[0]):
    print(f"\nüìç √âl√©ment {i+1}/{len(trajectories[0])}")
    print(f"   Type: {type(element)}")
    
    if isinstance(element, dict):
        print(f"   Cl√©s: {list(element.keys())}")
        
        # Observer l'√©tat
        if 'raw_obs' in element:
            obs = element['raw_obs']
            if isinstance(obs, np.ndarray):
                if obs.ndim > 0:
                    print(f"   √âtat: array de taille {len(obs)}")
                else:
                    print(f"   √âtat: scalaire numpy = {obs.item()}")
            else:
                print(f"   √âtat: {obs}")
        
        # Observer les actions l√©gales
        if 'raw_legal_actions' in element:
            print(f"   Actions l√©gales: {list(element['raw_legal_actions'])}")
    
    elif isinstance(element, (int, float, np.number)):
        print(f"   Valeur scalaire: {element}")
    
    elif isinstance(element, np.ndarray):
        print(f"   Array de shape: {element.shape}")
    
    else:
        print(f"   Contenu: {element}")

# Simuler 1000 mains
print("\n" + "=" * 60)
print("--- TOURNOI DE 1000 MAINS ---")
payoffs_total = tournament(env, 1000)
print(f"R√©sultats moyens sur 1000 mains:")
print(f"  Joueur 0 (Random): {payoffs_total[0]:+.2f}")
print(f"  Joueur 1 (Random): {payoffs_total[1]:+.2f}")

print("\n" + "=" * 60)
print("‚úÖ Exploration termin√©e !")
print("\nüìö Ce qu'on a appris:")
print("1. RLCard g√®re TOUT le jeu (r√®gles, pot, cartes, etc.)")
print("2. Les trajectoires ont une structure particuli√®re")
print("3. On re√ßoit toujours les actions l√©gales quand n√©cessaire")
print("4. Les agents Random sont √©quilibr√©s (moyenne ‚âà 0)")
print("\nüéØ Prochaine √©tape:")
print("   ‚Üí Cr√©er un agent XGBoost qui bat les Random !")
print("=" * 60)

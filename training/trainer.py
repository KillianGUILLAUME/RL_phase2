"""
Trainer agnostique pour agents de poker RL.
Compatible DQN, PPO, A2C, SAC, etc.
"""

import numpy as np
import torch
import rlcard
from rlcard.agents import RandomAgent
from rlcard.utils import reorganize
from pathlib import Path
from typing import List, Optional, Dict, Any
import time

from .config import FullTrainingConfig
from .callbacks import Callback, ProgressCallback, MetricsCallback


class PokerRLTrainer:
    """
    Trainer universel pour agents RL au poker.
    
    Principe : Le trainer est agnostique de l'algorithme RL utilis√©.
    Il suffit que l'agent impl√©mente :
      - feed(transition) : pour les algos on-policy (DQN, etc.)
      - ou update(trajectories) : pour les algos off-policy (PPO, etc.)
      - eval_step(state) : pour l'√©valuation
      - save_checkpoint(path) / load_checkpoint(path)
    """
    
    def __init__(
        self,
        agent,  # N'importe quel agent RL
        config: FullTrainingConfig,
        callbacks: Optional[List[Callback]] = None
    ):
        self.agent = agent
        self.config = config
        self.callbacks = callbacks or [ProgressCallback(), MetricsCallback()]
        
        # Setup environnement
        self.env = rlcard.make(
            self.config.env.game,
            config=self.config.env.to_dict()
        )
        
        # Setup device
        self.device = self.config.training.get_device()
        if hasattr(self.agent, 'device'):
            self.agent.device = self.device
        
        # Setup adversaires
        self.opponents = self._create_opponents()
        
        # Stats
        self.current_episode = 0
        self.stop_training = False
        
        print(f"üéÆ Trainer initialis√©")
        print(f"   Environnement : {self.config.env.game}")
        print(f"   Agent : {type(agent).__name__}")
        print(f"   Device : {self.device}")
    
    def _create_opponents(self) -> List:
        """Cr√©e les agents adversaires selon la config."""
        opponents = []
        
        for i in range(self.config.opponent.num_opponents):
            if self.config.opponent.type == 'random':
                opponents.append(RandomAgent(self.env.num_actions))
            
            elif self.config.opponent.type == 'xgboost':
                from agents.xgboost_agent import XGBoostRLCardAgent
                opponents.append(
                    XGBoostRLCardAgent(
                        model_path=self.config.opponent.model_path,
                        env=self.env
                    )
                )
            
            elif self.config.opponent.type == 'self_play':
                # En self-play, on met des copies de notre agent
                # (on les mettra √† jour p√©riodiquement)
                opponents.append(self.agent)
            
            else:
                raise ValueError(f"Type d'adversaire inconnu : {self.config.opponent.type}")
        
        return opponents
    
    def _trigger_callbacks(self, event: str, **kwargs):
        """D√©clenche un √©v√©nement sur tous les callbacks."""
        for callback in self.callbacks:
            method = getattr(callback, event, None)
            if method:
                method(self, **kwargs)
    
    def evaluate(self, n_episodes: Optional[int] = None) -> Dict[str, float]:
        """
        √âvalue l'agent contre des adversaires al√©atoires.
        Retourne un dict de m√©triques.
        """
        n_episodes = n_episodes or self.config.training.eval_episodes
        
        # Setup environnement d'√©valuation avec adversaires random
        eval_agents = [self.agent] + [RandomAgent(self.env.num_actions) 
                                       for _ in range(self.env.num_players - 1)]
        self.env.set_agents(eval_agents)
        
        rewards = []
        wins = 0
        
        for _ in range(n_episodes):
            trajectories, payoffs = self.env.run(is_training=False)
            rewards.append(payoffs[0])
            if payoffs[0] > 0:
                wins += 1
        
        metrics = {
            'avg_reward': np.mean(rewards),
            'std_reward': np.std(rewards),
            'min_reward': np.min(rewards),
            'max_reward': np.max(rewards),
            'win_rate': wins / n_episodes,
        }
        
        return metrics
    
    def train_episode(self) -> Dict[str, Any]:
        """
        Ex√©cute un √©pisode d'entra√Ænement.
        Retourne les m√©triques de l'√©pisode.
        """
        # Setup agents pour l'entra√Ænement
        training_agents = [self.agent] + self.opponents
        self.env.set_agents(training_agents)
        
        # Ex√©cution
        trajectories, payoffs = self.env.run(is_training=True)
        trajectories = reorganize(trajectories, payoffs)
        
        # Apprentissage (m√©thode agnostique)
        if hasattr(self.agent, 'feed'):
            # Style DQN : on feed transition par transition
            for ts in trajectories[0]:
                self.agent.feed(ts)
        
        elif hasattr(self.agent, 'update'):
            # Style PPO : on update avec toute la trajectoire
            self.agent.update(trajectories[0])
        
        else:
            raise NotImplementedError(
                "L'agent doit impl√©menter feed() ou update()"
            )
        
        # M√©triques de l'√©pisode
        metrics = {
            'reward': payoffs[0],
        }
        
        # Ajouter des m√©triques sp√©cifiques √† l'agent si disponibles
        if hasattr(self.agent, 'get_metrics'):
            agent_metrics = self.agent.get_metrics()
            metrics.update(agent_metrics)
        
        return metrics
    
    def train(self):
        """Lance l'entra√Ænement complet."""
        self._trigger_callbacks('on_training_start')
        
        try:
            for episode in range(self.config.training.num_episodes):
                if self.stop_training:
                    print("‚ö†Ô∏è  Entra√Ænement arr√™t√© par callback")
                    break
                
                self.current_episode = episode
                
                # Episode d'entra√Ænement
                self._trigger_callbacks('on_episode_start', episode=episode)
                metrics = self.train_episode()
                self._trigger_callbacks('on_episode_end', episode=episode, metrics=metrics)
                
                # √âvaluation p√©riodique
                if episode % self.config.training.eval_every == 0 and episode > 0:
                    eval_metrics = self.evaluate()
                    self._trigger_callbacks('on_evaluation_end', 
                                           episode=episode, 
                                           eval_metrics=eval_metrics)
                
                # Sauvegarde p√©riodique
                if episode % self.config.training.save_every == 0 and episode > 0:
                    self.save_checkpoint(episode)
        
        finally:
            self._trigger_callbacks('on_training_end')
    
    def save_checkpoint(self, episode: Optional[int] = None):
        """Sauvegarde un checkpoint."""
        if episode is None:
            episode = self.current_episode
        
        save_dir = Path(self.config.training.save_dir) / self.config.training.experiment_name
        save_path = save_dir / f'checkpoint_{episode}'
        save_path.mkdir(parents=True, exist_ok=True)
        
        # Sauvegarde de l'agent
        self.agent.save_checkpoint(str(save_path))
        
        # Sauvegarde de la config
        config_path = save_path / 'config.json'
        self.config.save(str(config_path))
        
        self._trigger_callbacks('on_checkpoint_save', 
                               episode=episode, 
                               save_path=str(save_path))
    
    def resume_training(self, checkpoint_path: str):
        """Reprend l'entra√Ænement depuis un checkpoint."""
        checkpoint_path = Path(checkpoint_path)
        
        # Charge l'agent
        if hasattr(self.agent, 'load_checkpoint'):
            self.agent.load_checkpoint(str(checkpoint_path))
            print(f"‚úÖ Agent charg√© depuis {checkpoint_path}")
        
        # TODO: Charger l'√©pisode actuel depuis metadata

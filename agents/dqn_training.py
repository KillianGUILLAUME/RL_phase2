import sys
sys.path.append('.')

import numpy as np

import rlcard
from rlcard.agents import DQNAgent, RandomAgent
from agents.dqn import SmartDQNAgent
from rlcard.utils import set_seed, tournament, reorganize, Logger
import torch

from agents.xgboost_agent import XGBoostRLCardAgent as XGBoostAgent 


# 1. Configurer l'environnement
env = rlcard.make('no-limit-holdem', config={'seed': 42})
hero_agent = XGBoostAgent(model_path='models/xgb/xgb_pluribus_V1.pkl', env = env)

def evaluate(agent, env, n_episodes=100):
    """Ã‰value l'agent contre des adversaires alÃ©atoires."""
    agents = [agent] + [RandomAgent(env.num_actions) for _ in range(env.num_players - 1)]
    env.set_agents(agents)
    rewards = []
    for _ in range(n_episodes):
        trajectories, payoffs = env.run(is_training=False)
        rewards.append(payoffs[0])
    return np.mean(rewards)


smart = True

if smart:
    agent = SmartDQNAgent(env, model_path='models/rl/dqn_smart')
else:
    agent = DQNAgent(
        num_actions=env.num_actions,
        state_shape=env.state_shape[0],
        mlp_layers=[64, 64],
        device=torch.device('cpu'),
        
        # ParamÃ¨tres d'apprentissage ajustÃ©s
        replay_memory_size=50000,      # MÃ©moire plus grande pour se souvenir des erreurs passÃ©es
        batch_size=32,
        replay_memory_init_size=500, # On le laisse observer un peu plus avant d'apprendre
        update_target_estimator_every=100,
        epsilon_decay_steps=20000 # Il explore longtemps avant de se figer
    )


agents = [agent] + [hero_agent for _ in range(5)]
env.set_agents(agents)

print("ğŸ‹ï¸â€â™‚ï¸ DÃ©but de l'entraÃ®nement du Sparring Partner DQN...")


for episode in range(1000):

    if episode % 1000 == 0:
        avg_reward = evaluate(agent, env)
        print(f"Episode {episode}: Avg Reward = {avg_reward:.2f}")
    
    trajectories, payoffs = env.run(is_training=True)

    trajectories = reorganize(trajectories, payoffs)
    
    for ts in trajectories[0]:
        agent.feed(ts)

    if episode % 250 == 0:
        print(f"   Episode {episode}/10000 terminÃ©...")

print("âœ… EntraÃ®nement terminÃ© !")

# 5. Sauvegarder ce modÃ¨le pour ne pas le rÃ©-entraÃ®ner Ã  chaque fois
import os
# save_path = 'models/rl/dqn_buddy'
# os.makedirs(save_path, exist_ok=True)
# agent.save_checkpoint(save_path)
# print(f"ğŸ’¾ Agent DQN sauvegardÃ© dans {save_path}")

# 5. Sauvegarde Finale
# RLCard sauvegarde souvent automatiquement si save_path est dÃ©fini, mais on force ici
# save_dir = 'models/rl/dqn_smart'
# os.makedirs(save_dir, exist_ok=True)
# # Astuce : DQNAgent a une mÃ©thode save_checkpoint, mais selon la version Ã§a change.
# # Le plus sÃ»r est de sauvegarder l'Ã©tat interne si la mÃ©thode n'existe pas.
# try:
#     agent.save_checkpoint(save_dir)
#     print(f"ğŸ’¾ Agent sauvegardÃ© dans {save_dir}")
# except AttributeError:
#     # Fallback si mÃ©thode non trouvÃ©e (dÃ©pend version RLCard)
#     torch.save(agent.q_estimator.qnet.state_dict(), os.path.join(save_dir, 'checkpoint_dqn.pt'))
#     print(f"ğŸ’¾ Poids du rÃ©seau sauvegardÃ©s manuellement dans {save_dir}